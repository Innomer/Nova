{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import librosa\n",
    "from data_loading import extract_features\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\"Shared feature extractor.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv1D(64, kernel_size=5, activation=None, input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        layers.Conv1D(128, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        layers.Conv1D(256, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Dense(64, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.Dropout(0.5)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_siamese_network(input_shape):\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    # Two inputs for the Siamese network\n",
    "    input_a = layers.Input(shape=input_shape)\n",
    "    input_b = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Shared embedding layers\n",
    "    embedding_a = base_network(input_a)\n",
    "    embedding_b = base_network(input_b)\n",
    "\n",
    "    # L2 normalization of embeddings\n",
    "    embedding_a = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(embedding_a)\n",
    "    embedding_b = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(embedding_b)\n",
    "\n",
    "    # Distance metric (Euclidean Distance)\n",
    "    distance = layers.Lambda(lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True) + 1e-8))([embedding_a, embedding_b])\n",
    "\n",
    "    \n",
    "    return Model(inputs=[input_a, input_b], outputs=distance)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1):\n",
    "    \"\"\"Contrastive loss as a custom loss function.\"\"\"\n",
    "    # y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)  # Reshape to [batch_size, 1]\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    squared_distance = tf.square(y_pred)\n",
    "    margin_distance = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean((1 - y_true) * squared_distance + y_true * margin_distance)\n",
    "\n",
    "def create_pairs(features, labels):\n",
    "    \"\"\"Generate pairs: (input_1, input_2, label).\"\"\"\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i, len(features)):\n",
    "            # Create a positive pair (same speaker)\n",
    "            if labels[i] == labels[j]:\n",
    "                pairs.append((features[i], features[j]))\n",
    "                pair_labels.append(0)  # Similar\n",
    "            # Create a negative pair (different speakers)\n",
    "            else:\n",
    "                pairs.append((features[i], features[j]))\n",
    "                pair_labels.append(1)  # Dissimilar\n",
    "    pairs, pair_labels = np.array(pairs), np.array(pair_labels)\n",
    "    indices = np.random.permutation(len(pairs))\n",
    "    pairs, pair_labels = pairs[indices], pair_labels[indices]\n",
    "    return np.array(pairs), np.array(pair_labels)\n",
    "\n",
    "# 6. Training\n",
    "def train_siamese_network():\n",
    "    siamese_net = create_siamese_network((80000,14))\n",
    "    print(\"Model Created\")\n",
    "    optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "\n",
    "    siamese_net.compile(optimizer=optimizer, loss=contrastive_loss, metrics=['accuracy'])\n",
    "    \n",
    "    for file in os.listdir(\"dataset\"):\n",
    "        if file.startswith(\"features_\"):\n",
    "            with open(f\"dataset/{file}\", \"rb\") as f:\n",
    "                features = pickle.load(f)\n",
    "            part_number=file.split(\".\")[0].split(\"_\")[-1]\n",
    "            with open(f\"dataset/labels_part_{part_number}.pkl\", \"rb\") as f:\n",
    "                labels=pickle.load(f)\n",
    "            print(\"Features Loaded for Part \", part_number)\n",
    "            # Break Features and Labels into 10 parts\n",
    "            features = np.array(features)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            # Normalize Features\n",
    "            features = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)\n",
    "\n",
    "            features = np.array_split(features, 20)\n",
    "            labels = np.array_split(labels, 20)\n",
    "            for features_part, labels_part in zip(features, labels):\n",
    "                pairs, pair_labels = create_pairs(features_part, labels_part)\n",
    "                pairs = np.array(pairs)\n",
    "                split = int(len(pairs) * 0.8)\n",
    "                train_pairs, test_pairs = pairs[:split], pairs[split:]\n",
    "                train_labels, test_labels = pair_labels[:split], pair_labels[split:]\n",
    "                history=siamese_net.fit(\n",
    "                    [train_pairs[:, 0], train_pairs[:, 1]], train_labels,\n",
    "                    validation_data=([test_pairs[:, 0], test_pairs[:, 1]], test_labels),\n",
    "                    epochs=1,\n",
    "                    batch_size=16,\n",
    "                    verbose=0\n",
    "                )\n",
    "                print(\"Model Trained\")\n",
    "            siamese_net.save(f\"weights/siamese_model_part_{part_number}.h5\")\n",
    "            print(\"Model Saved\")\n",
    "                \n",
    "                # Infer on 10 test pairs\n",
    "                # n1=random.randint(0, len(test_pairs)-11)\n",
    "                # n2=n1+10\n",
    "                # # Shuffle the test pairs and labels\n",
    "                # test_pairs = np.array(test_pairs)\n",
    "                # test_labels = np.array(test_labels)\n",
    "                # perm=np.random.permutation(len(test_pairs))\n",
    "                # test_pairs = test_pairs[perm]\n",
    "                # test_labels = test_labels[perm]\n",
    "                \n",
    "                # test_pairs = test_pairs[n1:n2]\n",
    "                # test_labels = test_labels[n1:n2]\n",
    "                # test_distances = siamese_net.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "                # test_loss=contrastive_loss(test_labels, test_distances)\n",
    "                # # roc_auc = roc_auc_score(test_labels, test_distances)\n",
    "                # threshold = 0.5\n",
    "                # test_predictions=[]\n",
    "                # for t in test_distances:\n",
    "                #     test_predictions.append(0 if t.mean()<threshold else 1)\n",
    "                # test_predictions = (test_distances > threshold).astype(int)\n",
    "                # test_predictions = test_distances.ravel() < threshold\n",
    "                \n",
    "                # precision=precision_score(test_labels, test_predictions)\n",
    "                # recall=recall_score(test_labels, test_predictions)\n",
    "                # f1=f1_score(test_labels, test_predictions)\n",
    "                \n",
    "                # test_predictions = np.squeeze(test_predictions)  # Remove extra dimensions\n",
    "                \n",
    "                # # print(test_distances, test_labels, test_predictions)\n",
    "                # # print(test_labels, test_predictions)\n",
    "                # # print(test_distances.shape, test_labels.shape, test_predictions.shape)\n",
    "                # print(f\"Test Loss: {test_loss}\")\n",
    "                # print(\"Test Accuracy: \", np.mean(test_predictions == test_labels))\n",
    "                # print(f\"ROC AUC: {roc_auc}\")\n",
    "                # print(f\"Precision: {precision}\")\n",
    "                # print(f\"Recall: {recall}\")\n",
    "                # print(f\"F1 Score: {f1}\")\n",
    "                # print(\"Model Evaluated\")                \n",
    "                \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Innomer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\Innomer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Innomer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Model Created\n",
      "Features Loaded for Part  0\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  1\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  10\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  100\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  11\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  12\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  13\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  14\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  15\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  16\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  17\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  18\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  19\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  2\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  20\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  21\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  22\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  23\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  24\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  25\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  26\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  27\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  28\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  29\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n",
      "Model Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "Model Saved\n",
      "Features Loaded for Part  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Run Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m train_siamese_network()\n",
      "Cell \u001b[1;32mIn[2], line 166\u001b[0m, in \u001b[0;36mtrain_siamese_network\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m     train_labels, test_labels \u001b[38;5;241m=\u001b[39m pair_labels[:split], pair_labels[split:]\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# print(\"Train and Test Pairs Created\")              \u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     history\u001b[38;5;241m=\u001b[39msiamese_net\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    167\u001b[0m         [train_pairs[:, \u001b[38;5;241m0\u001b[39m], train_pairs[:, \u001b[38;5;241m1\u001b[39m]], train_labels,\n\u001b[0;32m    168\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m([test_pairs[:, \u001b[38;5;241m0\u001b[39m], test_pairs[:, \u001b[38;5;241m1\u001b[39m]], test_labels),\n\u001b[0;32m    169\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    170\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m    171\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Trained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m siamese_net\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights/siamese_model_part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 7. Run Training\n",
    "model = train_siamese_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
